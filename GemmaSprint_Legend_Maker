{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1v9J5EN2cUmUWyNNK7TAHwGgDebq8FJQK","timestamp":1727885137732}],"gpuType":"T4","machine_shape":"hm","mount_file_id":"1v9J5EN2cUmUWyNNK7TAHwGgDebq8FJQK","authorship_tag":"ABX9TyNLAxR9lQioFZAqZVxpEX6N"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"ToI3ZREP5FJZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip3 install -q -U bitsandbytes\n","!pip3 install -q -U peft\n","!pip3 install -q -U trl\n","!pip3 install -q -U accelerate\n","!pip3 install -q -U datasets\n","!pip3 install -q -U transformers"],"metadata":{"id":"VZ9Y7THbA12a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from huggingface_hub import notebook_login\n","notebook_login()"],"metadata":{"id":"TuMo_H8A7PsO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from datasets import Dataset, load_dataset\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline, TrainingArguments\n","from peft import LoraConfig, PeftModel\n","from trl import SFTTrainer\n","from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, TextStreamer"],"metadata":{"id":"vk2UmB6Pz9R1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **기본 모델 Load**\n","출처 : https://huggingface.co/unsloth/gemma-2-2b-it"],"metadata":{"id":"gfRxWRv9e0WA"}},{"cell_type":"code","source":["BASE_MODEL = \"unsloth/gemma-2b-it\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(\n","    \"unsloth/gemma-2b-it\",\n",")\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    \"unsloth/gemma-2b-it\",\n","    torch_dtype=torch.bfloat16,\n","    device_map='auto',\n",")"],"metadata":{"id":"_cLrkfexHgTA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **기본모델 Test**\n","**모델 성능확인 결과**\n","1. 한국어로 이야기를 만들 때 내용이 부자연스러움\n","2. 프롬프트를 조정으로 출력물의 완성도를 높일 수  있으나 자연스러운 스토리라인이 만들어지지 않음\n","3. 영어 작문은 정상 작동하는 것으로 확인됨"],"metadata":{"id":"YzY_o_oZGdDU"}},{"cell_type":"markdown","source":["1. 기본 test"],"metadata":{"id":"w4jxWx3lt2Nl"}},{"cell_type":"code","source":["import google.generativeai as genai\n","\n","# Google API 키 설정 (실제 키로 교체 필요)\n","#genai.configure(api_key=\"YOUR_API_KEY\")\n","\n","# Gemma 모델 설정\n","# model = genai.GenerativeModel('gemma2-2b-it')\n","\n","def get_user_input():\n","    \"\"\"사용자로부터 일상적인 사건을 입력받는 함수\"\"\"\n","    return input(\"일상적인 사건을 입력해주세요: \")\n","\n","def create_prompt(event):\n","    \"\"\"입력받은 사건을 바탕으로 프롬프트를 생성하는 함수\"\"\"\n","    return f\"\"\"\n","    다음 주제에 대한 이야기를 작성해주세요.\n","    주제 : \"{event}\"\n","    \"\"\"\n","def generate_story(prompt):\n","    \"\"\"Gemma 모델을 사용하여 이야기를 생성하는 함수\"\"\"\n","    # Use the appropriate method for the Hugging Face model\n","    # Set max_new_tokens to a value that allows the model to generate enough text\n","    # Move the input tensors to the same device as the model\n","    response = model.generate(**tokenizer(prompt, return_tensors=\"pt\").to(model.device), max_new_tokens=512)\n","    return tokenizer.decode(response[0])\n","\n","def main():\n","    event = get_user_input()\n","    prompt = create_prompt(event)\n","    story = generate_story(prompt)\n","    print(\"\\n생성된 이야기:\\n\")\n","    print(story)\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"XD6mYtAkI8vh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. 프롬프트 수정"],"metadata":{"id":"5tu6s3FVt5X6"}},{"cell_type":"code","source":["import google.generativeai as genai\n","\n","# Google API 키 설정 (실제 키로 교체 필요)\n","#genai.configure(api_key=\"YOUR_API_KEY\")\n","\n","# Gemma 모델 설정\n","# model = genai.GenerativeModel('gemma2-2b-it')\n","\n","def get_user_input():\n","    \"\"\"사용자로부터 일상적인 사건을 입력받는 함수\"\"\"\n","    return input(\"일상적인 사건을 입력해주세요: \")\n","\n","def create_prompt(event):\n","    \"\"\"입력받은 사건을 바탕으로 프롬프트를 생성하는 함수\"\"\"\n","    return f\"\"\"\n","    다음 주제에 대한 이야기를 작성해주세요.\n","    주제 : \"{event}\"\n","    언어 : 한국어\n","    분량 : 200단어 내외\n","    \"\"\"\n","def generate_story(prompt):\n","    \"\"\"Gemma 모델을 사용하여 이야기를 생성하는 함수\"\"\"\n","    # Use the appropriate method for the Hugging Face model\n","    # Set max_new_tokens to a value that allows the model to generate enough text\n","    # Move the input tensors to the same device as the model\n","    response = model.generate(**tokenizer(prompt, return_tensors=\"pt\").to(model.device), max_new_tokens=512)\n","    return tokenizer.decode(response[0])\n","\n","def main():\n","    event = get_user_input()\n","    prompt = create_prompt(event)\n","    story = generate_story(prompt)\n","    print(\"\\n생성된 이야기:\\n\")\n","    print(story)\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"gllWTt3IGvWE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. 영어기반 작문유도 프롬프트 수정"],"metadata":{"id":"_Mtwr9Oft8E0"}},{"cell_type":"code","source":["import google.generativeai as genai\n","\n","# Google API 키 설정 (실제 키로 교체 필요)\n","#genai.configure(api_key=\"YOUR_API_KEY\")\n","\n","# Gemma 모델 설정\n","# model = genai.GenerativeModel('gemma2-2b-it')\n","\n","def get_user_input():\n","    \"\"\"사용자로부터 일상적인 사건을 입력받는 함수\"\"\"\n","    return input(\"일상적인 사건을 입력해주세요: \")\n","\n","def create_prompt(event):\n","    \"\"\"입력받은 사건을 바탕으로 프롬프트를 생성하는 함수\"\"\"\n","    return f\"\"\"\n","    다음 주제에 대한 이야기를 작성해주세요.\n","    주제 : \"{event}\"\n","    언어 : 한국어\n","\n","    1. 200단어 내외의 영어 이야기 작성\n","    2. 영어를 한국어로 번역해서 출력\n","    \"\"\"\n","def generate_story(prompt):\n","    \"\"\"Gemma 모델을 사용하여 이야기를 생성하는 함수\"\"\"\n","    # Use the appropriate method for the Hugging Face model\n","    # Set max_new_tokens to a value that allows the model to generate enough text\n","    # Move the input tensors to the same device as the model\n","    response = model.generate(**tokenizer(prompt, return_tensors=\"pt\").to(model.device), max_new_tokens=1000)\n","    return tokenizer.decode(response[0])\n","\n","def main():\n","    event = get_user_input()\n","    prompt = create_prompt(event)\n","    story = generate_story(prompt)\n","    print(\"\\n생성된 이야기:\\n\")\n","    print(story)\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"6tTt_q7uvo5n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **세가지 종류의 후보 데이터를 활용하여 Gemma Fine-tuning 진행**\n","1. (AI허브) 다양한 문화콘텐츠 스토리 데이터\n","- 출처 : https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=data&dataSetSn=71562\n","\n","2. (huggingface) 네이버 뉴스 요약 데이터\n","- 출처 : https://huggingface.co/datasets/daekeun-ml/naver-news-summarization-ko\n","\n","3. (AI허브) 방송 콘텐츠 대본 요약 데이터\n","- 출처 : https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=data&dataSetSn=591\n","\n","**목표 : 요약본에서 원본을 만들어 내도록 데이터를 학습하여 한국어 생성 성능 향상**"],"metadata":{"id":"Osy6UtMhR4ps"}},{"cell_type":"markdown","source":["# **데이터1**\n","1. AI허브 : 다양한 문화콘텐츠 스토리 데이터 (storyline -> script)"],"metadata":{"id":"NV9MU4fsgE78"}},{"cell_type":"markdown","source":["- 데이터1 전처리"],"metadata":{"id":"KASDCpt4mwwG"}},{"cell_type":"code","source":["import os\n","import zipfile\n","\n","def unzip_all(dir, out_dir):\n","  for f in os.listdir(dir):\n","    if f.endswith(\".zip\"):\n","      f_dir = os.path.join(dir, f)\n","      with zipfile.ZipFile(f_dir, 'r') as zip_ref:\n","        zip_ref.extractall(out_dir)\n","      print(f\"{f} 압축 해제 완료\")\n","\n","dir_path = \"/content/drive/MyDrive/story_data/Training/Labeled\"\n","out_path = \"/content/drive/MyDrive/story_data/data_all\"\n","unzip_all(dir_path, out_path)\n","dir_path = \"/content/drive/MyDrive/story_data/Validation/Labeled\"\n","out_path = \"/content/drive/MyDrive/story_data/data_all\"\n","unzip_all(dir_path, out_path)"],"metadata":{"id":"A9tyO3cojFPA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import json\n","\n","# JSON 파일이 있는 디렉토리 경로 설정\n","dir_path = '/content/drive/MyDrive/story_data/data_all'\n","\n","# 모든 처리된 데이터를 저장할 리스트\n","all_processed_data = []\n","\n","# 디렉토리 내 모든 JSON 파일 처리\n","for filename in os.listdir(dir_path):\n","    if filename.endswith('.json'):\n","        input_file_path = os.path.join(dir_path, filename)\n","\n","        with open(input_file_path, 'r', encoding='utf-8') as f:\n","            data = json.load(f)\n","\n","        # 각 파일의 데이터 처리\n","        for story in data[\"units\"]:\n","            input_text = story['storyline']\n","            output_text = ' '.join([script['content'] for script in story['story_scripts']])\n","            all_processed_data.append({'input': input_text, 'output': output_text})\n","\n","# 처리된 전체 데이터 확인\n","print(f\"총 처리된 데이터 수: {len(all_processed_data)}\")\n","\n","# QLora fine-tuning을 위한 데이터 형식으로 변환\n","qlora_data = [\n","    {\n","        \"prompt\": f\" 다음 상황을 바탕으로 이야기를 만들어주세요: {item['input']}\",\n","        \"completion\": item['output']\n","    }\n","    for item in all_processed_data\n","]\n","\n","# QLora 데이터를 JSON 파일로 저장\n","output_file_path = '/content/drive/MyDrive/story_data/qlora_finetuning_data.json'\n","with open(output_file_path, 'w', encoding='utf-8') as f:\n","    json.dump(qlora_data, f, ensure_ascii=False, indent=2)\n"],"metadata":{"id":"55TBR3e6CpaV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_prompt(example):\n","    prompt_list = []\n","    for i in range(len(example['completion'])):\n","        prompt_list.append(r\"\"\"<bos><start_of_turn>user\n","        {}<end_of_turn>\n","        <start_of_turn>model\n","        {}<end_of_turn><eos>\"\"\".format(example['prompt'][i], example['completion'][i]))\n","    return prompt_list"],"metadata":{"id":"HChMfscbPSl2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","dataset = load_dataset('json', data_files='/content/drive/MyDrive/story_data/qlora_finetuning_data.json')\n","\n","train_data = dataset['train']"],"metadata":{"id":"054rzio5PTNV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data[0]"],"metadata":{"id":"cWUEzsetBdnb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 데이터1 학습"],"metadata":{"id":"tdPysKhYhcWA"}},{"cell_type":"code","source":["from transformers import TrainingArguments, Trainer\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n","from peft import LoraConfig, get_peft_model\n","\n","\n","lora_config = LoraConfig(\n","    r=6,\n","    lora_alpha = 8,\n","    lora_dropout = 0.05,\n","    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.float16\n",")\n","\n","BASE_MODEL = \"unsloth/gemma-2b-it\"\n","\n","model = AutoModelForCausalLM.from_pretrained(BASE_MODEL,\n","                                             quantization_config=bnb_config,\n","                                             device_map='auto',\n","                                             )\n","tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n","tokenizer.padding_side = 'right'\n","\n","\n","# Trainer 초기화\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=train_data,\n","    max_seq_length=512,\n","    args=TrainingArguments(\n","        output_dir=\"/content/drive/MyDrive/story_data/outputs_unsloth\",\n","#        num_train_epochs = 1,\n","        max_steps=3000,\n","        per_device_train_batch_size=1,\n","        gradient_accumulation_steps=4,\n","        optim=\"paged_adamw_8bit\",\n","        warmup_steps=100,\n","        learning_rate=2e-4,\n","        fp16=True,\n","        logging_steps=100,\n","        weight_decay = 0.01,\n","        push_to_hub=False,\n","        report_to='none',\n","\n","        lr_scheduler_type = \"linear\",\n","        seed = 777,\n","    ),\n","    peft_config=lora_config,\n","    formatting_func=generate_prompt,\n",")\n"," # 학습 실행\n","trainer.train()"],"metadata":{"id":"y9EXaMEBcvqb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 데이터1 결과확인"],"metadata":{"id":"ZfKtZ_f6hmeY"}},{"cell_type":"code","source":["ADAPTER_MODEL = \"/content/drive/MyDrive/story_data/outputs_unsloth/checkpoint-3000\""],"metadata":{"id":"fw8mGappiBZE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 학습된 weight\n","!ls -alh /content/drive/MyDrive/story_data/outputs_unsloth/checkpoint-3000"],"metadata":{"id":"Qkm-FuHjwI2-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["BASE_MODEL = \"unsloth/gemma-2b-it\"\n","model_f1 = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map='auto', torch_dtype=torch.float16)\n","model_f1 = PeftModel.from_pretrained(model_f1, ADAPTER_MODEL, device_map='auto', torch_dtype=torch.float16)\n","\n","model_f1 = model_f1.merge_and_unload()"],"metadata":{"id":"Uj73C3EZwOqt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import google.generativeai as genai\n","\n","# Google API 키 설정 (실제 키로 교체 필요)\n","#genai.configure(api_key=\"YOUR_API_KEY\")\n","\n","# Gemma 모델 설정\n","# model = genai.GenerativeModel('gemma2-2b-it')\n","\n","def get_user_input():\n","    \"\"\"사용자로부터 일상적인 사건을 입력받는 함수\"\"\"\n","    return input(\"일상적인 사건을 입력해주세요: \")\n","\n","def create_prompt(event):\n","    \"\"\"입력받은 사건을 바탕으로 프롬프트를 생성하는 함수\"\"\"\n","    return f\"\"\"\n","    다음 주제에 대한 이야기를 작성해주세요.\n","    주제 : \"{event}\"\n","    \"\"\"\n","def generate_story(prompt):\n","    \"\"\"Gemma 모델을 사용하여 이야기를 생성하는 함수\"\"\"\n","    # Use the appropriate method for the Hugging Face model\n","    # Set max_new_tokens to a value that allows the model to generate enough text\n","    # Move the input tensors to the same device as the model\n","    response = model_f1.generate(**tokenizer(prompt, return_tensors=\"pt\").to(model_f1.device), max_new_tokens=512)\n","    return tokenizer.decode(response[0])\n","\n","def main():\n","    event = get_user_input()\n","    prompt = create_prompt(event)\n","    story = generate_story(prompt)\n","    print(\"\\n생성된 이야기:\\n\")\n","    print(story)\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"iiCLf-V5yt7U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 부자연스러운 형태가 남아있으나 기존 대비 자연스러운 스토리 전개가 확인됨. 스크립트 형식으로 출력."],"metadata":{"id":"1gQ786MxBovN"}},{"cell_type":"markdown","source":["# **학습데이터2**\n","1. Naver new summarization (summarization -> document)"],"metadata":{"id":"DLK50PM-q-Oz"}},{"cell_type":"markdown","source":["- 데이터2 전처리"],"metadata":{"id":"kVfQeMJ7CG8M"}},{"cell_type":"code","source":["from datasets import load_dataset\n","dataset = load_dataset(\"daekeun-ml/naver-news-summarization-ko\")"],"metadata":{"id":"j_QWk0f7Ka3Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_prompt_news(example):\n","    prompt_list = []\n","    for i in range(len(example['summary'])):\n","        prompt_list.append(r\"\"\"<bos><start_of_turn>user\n","        다음 상황을 바탕으로 이야기를 만들어주세요 : {}<end_of_turn>\n","        <start_of_turn>model\n","        {}<end_of_turn><eos>\"\"\".format(example['summary'][i], example['document'][i]))\n","    return prompt_list"],"metadata":{"id":"be0mXhP5q_pF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data = dataset['train']"],"metadata":{"id":"tAVUvNx45-eL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(generate_prompt_news(train_data[:1])[0])"],"metadata":{"id":"mEr5YWJ26HKD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 데이터2 학습"],"metadata":{"id":"sMUrGR0GDnIO"}},{"cell_type":"code","source":["lora_config = LoraConfig(\n","    r=6,\n","    lora_alpha = 8,\n","    lora_dropout = 0.05,\n","    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.float16\n",")\n","\n","BASE_MODEL = \"unsloth/gemma-2b-it\"\n","\n","model = AutoModelForCausalLM.from_pretrained(BASE_MODEL,\n","                                             quantization_config=bnb_config,\n","                                             device_map='auto',\n","                                             )\n","tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n","tokenizer.padding_side = 'right'\n","\n","\n","# Trainer 초기화\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=train_data,\n","    max_seq_length=512,\n","    args=TrainingArguments(\n","        output_dir=\"/content/drive/MyDrive/story_data/outputs_news_unsloth\",\n","        max_steps=3000,\n","        per_device_train_batch_size=1,\n","        gradient_accumulation_steps=4,\n","        optim=\"paged_adamw_8bit\",\n","        warmup_steps=100,\n","        learning_rate=2e-4,\n","        fp16=True,\n","        logging_steps=100,\n","        weight_decay = 0.01,\n","        push_to_hub=False,\n","        report_to='none',\n","\n","        lr_scheduler_type = \"linear\",\n","        seed = 777,\n","    ),\n","    peft_config=lora_config,\n","    formatting_func=generate_prompt_news,\n",")\n","\n","# 학습 실행\n","trainer.train()"],"metadata":{"id":"D8s7GnzFXmKH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" ADAPTER_MODEL = \"/content/drive/MyDrive/story_data/outputs_news_unsloth/checkpoint-5000\""],"metadata":{"id":"8rqMGYY2X7xL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 데이터2 결과확인"],"metadata":{"id":"O0D2bhadDyeK"}},{"cell_type":"code","source":["# 학습된 weight\n","!ls -alh /content/drive/MyDrive/story_data/outputs_news_unsloth/checkpoint-5000"],"metadata":{"id":"j_H8_2zckY72"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["BASE_MODEL = \"unsloth/gemma-2b-it\"\n","model_f2 = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map='auto', torch_dtype=torch.float16)\n","model_f2 = PeftModel.from_pretrained(model_f2, ADAPTER_MODEL, device_map='auto', torch_dtype=torch.float16)\n","model_f2 = model_f2.merge_and_unload()"],"metadata":{"id":"KJjDL87EX-mM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import google.generativeai as genai\n","\n","# Google API 키 설정 (실제 키로 교체 필요)\n","#genai.configure(api_key=\"YOUR_API_KEY\")\n","\n","# Gemma 모델 설정\n","# model = genai.GenerativeModel('gemma2-2b-it')\n","\n","def get_user_input():\n","    \"\"\"사용자로부터 일상적인 사건을 입력받는 함수\"\"\"\n","    return input(\"일상적인 사건을 입력해주세요: \")\n","\n","def create_prompt(event):\n","    \"\"\"입력받은 사건을 바탕으로 프롬프트를 생성하는 함수\"\"\"\n","    return f\"\"\"\n","    다음 주제에 대한 뉴스를 작성해주세요.\n","    주제 : \"{event}\"\n","    \"\"\"\n","def generate_story(prompt):\n","    \"\"\"Gemma 모델을 사용하여 이야기를 생성하는 함수\"\"\"\n","    # Use the appropriate method for the Hugging Face model\n","    # Set max_new_tokens to a value that allows the model to generate enough text\n","    # Move the input tensors to the same device as the model\n","    response = model_f2.generate(**tokenizer(prompt, return_tensors=\"pt\").to(model_f2.device), max_new_tokens=512)\n","    return tokenizer.decode(response[0])\n","\n","def main():\n","    event = get_user_input()\n","    prompt = create_prompt(event)\n","    story = generate_story(prompt)\n","    print(\"\\n생성된 이야기:\\n\")\n","    print(story)\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"bXUw3HjAkd-Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- Fine tuning이 잘 되지 않은 것으로 보임"],"metadata":{"id":"fOtEn4ughyXd"}},{"cell_type":"markdown","source":["# **학습데이터3**\n","3. AI허브 : 방송 콘텐츠 대본 요약 데이터 (summary -> passage)"],"metadata":{"id":"YKFsAjMOhzz-"}},{"cell_type":"markdown","source":["- 데이터3 전처리"],"metadata":{"id":"ulbXOsv3D7o3"}},{"cell_type":"code","source":["import os\n","import zipfile\n","\n","def unzip_all(dir, out_dir):\n","  \"\"\"\n","  지정된 디렉토리에 있는 모든 zip 파일을 압축 해제합니다.\n","\n","  Args:\n","    디렉토리_경로: 압축 해제할 zip 파일이 있는 디렉토리 경로입니다.\n","  \"\"\"\n","  for f in os.listdir(dir):\n","    if f.endswith(\".zip\"):\n","      f_dir = os.path.join(dir, f)\n","      with zipfile.ZipFile(f_dir, 'r') as zip_ref:\n","        zip_ref.extractall(out_dir)\n","      print(f\"{f} 압축 해제 완료\")\n","\n","dir_path = \"/content/drive/MyDrive/broadcast\"\n","out_path = \"/content/drive/MyDrive/broadcast\"\n","\n","unzip_all(dir_path, out_path)\n"],"metadata":{"id":"X9q9pTObiISc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import json\n","\n","# JSON 파일이 있는 디렉토리 경로 설정\n","dir_path = '/content/drive/MyDrive/broadcast'\n","\n","# 모든 처리된 데이터를 저장할 리스트\n","all_processed_data = []\n","\n","# 디렉토리 내 모든 JSON 파일 처리\n","for dirname1 in os.listdir(dir_path):\n","    for dirname2 in os.listdir(os.path.join(dir_path, dirname1)):\n","        for filename in os.listdir(os.path.join(dir_path, dirname1,dirname2)):\n","            if filename.endswith('.json'):\n","                input_file_path = os.path.join(dir_path, dirname1, dirname2, filename)\n","                with open(input_file_path, 'r', encoding='utf-8') as f:\n","                    data = json.load(f)\n","\n","                # 각 파일의 데이터 처리\n","                input_text = data[\"Annotation\"][\"Summary1\"]\n","                output_text = data[\"Meta\"][\"passage\"]\n","                all_processed_data.append({'input': input_text, 'output': output_text})\n","\n","# 처리된 전체 데이터 확인\n","print(f\"총 처리된 데이터 수: {len(all_processed_data)}\")\n","\n","# QLora fine-tuning을 위한 데이터 형식으로 변환\n","qlora_data = [\n","    {\n","        \"prompt\": f\" 다음 상황을 바탕으로 이야기를 만들어주세요: {item['input']}\",\n","        \"completion\": item['output']\n","    }\n","    for item in all_processed_data\n","]\n","\n","# QLora 데이터를 JSON 파일로 저장\n","output_file_path = '/content/drive/MyDrive/broadcast/qlora_finetuning_data.json'\n","with open(output_file_path, 'w', encoding='utf-8') as f:\n","    json.dump(qlora_data, f, ensure_ascii=False, indent=2)\n","\n","print(f\"QLora fine-tuning 데이터가 {output_file_path}에 저장되었습니다.\")"],"metadata":{"id":"VSRUwfcjlCy_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 데이터3 학습"],"metadata":{"id":"Hw16vg7fMC5f"}},{"cell_type":"code","source":["def generate_prompt(example):\n","    prompt_list = []\n","    for i in range(len(example['completion'])):\n","        prompt_list.append(r\"\"\"<bos><start_of_turn>user\n","        {}<end_of_turn>\n","        <start_of_turn>model\n","        {}<end_of_turn><eos>\"\"\".format(example['prompt'][i], example['completion'][i]))\n","    return prompt_list\n","\n","from datasets import load_dataset\n","\n","dataset = load_dataset('json', data_files='/content/drive/MyDrive/broadcast/qlora_finetuning_data.json')\n","\n","train_data = dataset['train']"],"metadata":{"id":"iuczQFp4MFZl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# QLoRA 설정\n","lora_config = LoraConfig(\n","    r=6,\n","    lora_alpha = 8,\n","    lora_dropout = 0.05,\n","    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.float16\n",")"],"metadata":{"id":"6p7dM9EyWHZ5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data[0]"],"metadata":{"id":"Kt7vC1qlcJrl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델 로드\n","BASE_MODEL = \"unsloth/gemma-2b-it\"\n","\n","model = AutoModelForCausalLM.from_pretrained(BASE_MODEL,\n","                                             quantization_config=bnb_config,\n","                                             device_map='auto',\n","                                             )\n","tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n","tokenizer.padding_side = 'right'\n","\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=train_data,\n","    max_seq_length=512,\n","    args=TrainingArguments(\n","        output_dir=\"/content/drive/MyDrive/broadcast/output_3\",\n","        max_steps=2000,\n","        per_device_train_batch_size=1,\n","        gradient_accumulation_steps=4,\n","        optim=\"paged_adamw_8bit\",\n","        warmup_steps=50,\n","        learning_rate=2e-4,\n","        fp16=True,\n","        logging_steps=100,\n","        push_to_hub=False,\n","        report_to='none',\n","    ),\n","    peft_config=lora_config,\n","    formatting_func=generate_prompt,\n",")\n","# 학습 실행\n","trainer.train()"],"metadata":{"id":"84GOaipKNFk9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 데이터3 결과확인"],"metadata":{"id":"kGUggfCSEBGn"}},{"cell_type":"code","source":[" ADAPTER_MODEL = \"/content/drive/MyDrive/broadcast/output_3/checkpoint-500\""],"metadata":{"id":"s8E0-9mpcnCa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["BASE_MODEL = \"unsloth/gemma-2b-it\"\n","model_f3 = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map='auto', torch_dtype=torch.float16)\n","model_f3 = PeftModel.from_pretrained(model_f3, ADAPTER_MODEL, device_map='auto', torch_dtype=torch.float16)\n","\n","model_f3 = model_f3.merge_and_unload()"],"metadata":{"id":"A7Ntz4tlcr-I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import google.generativeai as genai\n","\n","# Google API 키 설정 (실제 키로 교체 필요)\n","#genai.configure(api_key=\"YOUR_API_KEY\")\n","\n","# Gemma 모델 설정\n","# model = genai.GenerativeModel('gemma2-2b-it')\n","\n","def get_user_input():\n","    \"\"\"사용자로부터 일상적인 사건을 입력받는 함수\"\"\"\n","    return input(\"일상적인 사건을 입력해주세요: \")\n","\n","def create_prompt(event):\n","    \"\"\"입력받은 사건을 바탕으로 프롬프트를 생성하는 함수\"\"\"\n","    return f\"\"\"\n","    다음 주제에 대한 이야기를 작성해주세요.\n","    주제 : {event}\n","    \"\"\"\n","def generate_story(prompt):\n","    \"\"\"Gemma 모델을 사용하여 이야기를 생성하는 함수\"\"\"\n","    # Use the appropriate method for the Hugging Face model\n","    # Set max_new_tokens to a value that allows the model to generate enough text\n","    # Move the input tensors to the same device as the model\n","    response = model_f3.generate(**tokenizer(prompt, return_tensors=\"pt\").to(model_f3.device), max_new_tokens=512)\n","    return tokenizer.decode(response[0])\n","\n","def main():\n","    event = get_user_input()\n","    prompt = create_prompt(event)\n","    story = generate_story(prompt)\n","    print(\"\\n생성된 이야기:\\n\")\n","    print(story)\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"dOgzjaPMcxKP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- Fine tuning이 잘 되지 않은 것으로 보임"],"metadata":{"id":"Vz6q54rDh4lu"}},{"cell_type":"markdown","source":["# **결과요약**\n","**1. (AI허브) 다양한 문화콘텐츠 스토리 데이터**\n","\n","- 기본모형 대비 자연스러운 결과를 보여줌. 스크립트 형식으로 출력.\n","\n","**2. (huggingface) 네이버 뉴스 요약 데이터**\n","\n","- 뉴스형식으로 출력을 시작하지만 정상적인 output이 출력되지 않음.\n","\n","**3. (AI허브) 방송 콘텐츠 대본 요약 데이터**\n","\n","- 정상 출력되지 않음. 방송 콘텐츠의 텍스트 형식에 대한 추가 전처리 과정이 필요."],"metadata":{"id":"II2JjVGZctpa"}},{"cell_type":"markdown","source":["# ** model_f1을 기준으로 추가 결과 확인**"],"metadata":{"id":"mVxMW1Acdj5p"}},{"cell_type":"code","source":["ADAPTER_MODEL = \"/content/drive/MyDrive/story_data/outputs_unsloth/checkpoint-3000\""],"metadata":{"id":"9zMyDhdbd6Jq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["BASE_MODEL = \"unsloth/gemma-2b-it\"\n","model_f1 = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map='auto', torch_dtype=torch.float16)\n","model_f1 = PeftModel.from_pretrained(model_f1, ADAPTER_MODEL, device_map='auto', torch_dtype=torch.float16)\n","\n","model_f1 = model_f1.merge_and_unload()"],"metadata":{"id":"zdlAYgdLd1ND"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import google.generativeai as genai\n","\n","# Google API 키 설정 (실제 키로 교체 필요)\n","#genai.configure(api_key=\"YOUR_API_KEY\")\n","\n","# Gemma 모델 설정\n","# model = genai.GenerativeModel('gemma2-2b-it')\n","\n","def get_user_input():\n","    \"\"\"사용자로부터 일상적인 사건을 입력받는 함수\"\"\"\n","    return input(\"일상적인 사건을 입력해주세요: \")\n","\n","def create_prompt(event):\n","    \"\"\"입력받은 사건을 바탕으로 프롬프트를 생성하는 함수\"\"\"\n","    return f\"\"\"\n","    다음 주제에 대한 이야기를 작성해주세요.\n","    주제 : \"{event}\"\n","    \"\"\"\n","def generate_story(prompt):\n","    \"\"\"Gemma 모델을 사용하여 이야기를 생성하는 함수\"\"\"\n","    # Use the appropriate method for the Hugging Face model\n","    # Set max_new_tokens to a value that allows the model to generate enough text\n","    # Move the input tensors to the same device as the model\n","    response = model_f1.generate(**tokenizer(prompt, return_tensors=\"pt\").to(model_f1.device), max_new_tokens=512)\n","    return tokenizer.decode(response[0])\n","\n","def main():\n","    event = get_user_input()\n","    prompt = create_prompt(event)\n","    story = generate_story(prompt)\n","    print(\"\\n생성된 이야기:\\n\")\n","    print(story)\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"-RvO4cpIdXyc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"fDa27YMDeILi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **결론**\n","- Fine Tuning을 통해 기존보다 자연스러운 한국어 이야기를 구현하게 되었지만, 여전히 입력 텍스트에 대한 맥락 이해가 부족하고 한국어 문법에서 오류를 보이는 경우가 많다.\n","\n","- 대규모 학습을 통해 개선이 가능할 듯하다."],"metadata":{"id":"iDs4qU6xsv9Y"}}]}